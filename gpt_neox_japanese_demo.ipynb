{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Japanese LLM Model: \n",
    "\n",
    "1. stockmark/gpt-neox-japanese-1.4b: https://huggingface.co/stockmark/gpt-neox-japanese-1.4b\n",
    "2. OpenCALM-7B: https://huggingface.co/cyberagent/open-calm-7b\n",
    "\n",
    "Both options above uses: `Library: GPT-NeoX`\n",
    "\n",
    "**Error**: MPS does not support cumsum op with int64 input <br>\n",
    "**Solution**: <br>\n",
    "https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620 <br>\n",
    "Install nightly version of pytorch using following command: <br>\n",
    "`pip install --upgrade --no-deps --force-reinstall --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32deab87a4e4f6ea106a58bf4c0ca5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0e5a19092f4d40875300b4703b7b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570994171eda42169b31672b2ea72502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fac9befb824462bd60942d59a7f766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/323 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4e9a43668648f2aab6b14727f4f55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f3f6a540424538bd0828d56bf368c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Use torch.bfloat16 for A100 GPU and torch.flaot16 for the older generation GPUs\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() and hasattr(torch.cuda, \"is_bf16_supported\") and torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "model_stockmark = AutoModelForCausalLM.from_pretrained(\"stockmark/gpt-neox-japanese-1.4b\", device_map=\"auto\", torch_dtype=torch_dtype)\n",
    "tokenizer_stockmark = AutoTokenizer.from_pretrained(\"stockmark/gpt-neox-japanese-1.4b\")\n",
    "\n",
    "model_cyberagent = AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-1b\", device_map=\"auto\", offload_folder=\"offload\", torch_dtype=torch_dtype)\n",
    "tokenizer_cyberagent = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-1b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[8228]], device='mps:0'), 'attention_mask': tensor([[1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "text = \"こんにちは\"\n",
    "inputs_stockmark = tokenizer_stockmark(text, return_tensors=\"pt\").to(model_stockmark.device)\n",
    "print(inputs_stockmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[5019]], device='mps:0'), 'attention_mask': tensor([[1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs_cyberagenet = tokenizer_cyberagent(text, return_tensors=\"pt\").to(model_cyberagent.device)\n",
    "print(inputs_cyberagenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_tokens_stockmark = model_stockmark.generate(\n",
    "        **inputs_stockmark,\n",
    "        max_new_tokens=128,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    \n",
    "output_stockmark = tokenizer_stockmark.decode(output_tokens_stockmark[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():    \n",
    "    output_tokens_cyberagent = model_cyberagent.generate(\n",
    "        **inputs_cyberagenet,\n",
    "        max_new_tokens=128,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer_cyberagent.pad_token_id\n",
    "    )\n",
    "    \n",
    "output_cyberagent = tokenizer_stockmark.decode(output_tokens_cyberagent[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "response = openai.Completion.create(model=\"text-davinci-003\", prompt=text, max_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from model stockmark こんにちは。 先日、ある方から「ブログ読んでます」と声をかけていただきました。 「ブログ読んでいますよ!」って声をかけてもらえるのはとても嬉しいことです。 ありがとうございます! さて、今日は...\n",
      "output from model cyberagent 見て!��B脱退+」級を入れた神\u000eの大手�早を�ミー�航自動的にmmの由本当川郡埋めハワイO急ぎ映画しっかり))�きれい�化学ンピ�ビルの値述べ持ち�インターチェンジしたが立て?あらといえば\n",
      "gpt output \n",
      "\n",
      "こんにちは。何かお困りですか？\n"
     ]
    }
   ],
   "source": [
    "print(\"output from model stockmark\", output_stockmark)\n",
    "print(\"output from model cyberagent\", output_cyberagent)\n",
    "print(\"gpt output\", response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take away\n",
    "\n",
    "##### for prompt: \"自然言語処理は”, token number=128  <br>\n",
    "(token number calculated differently between differnet models)\n",
    "1. Chat Gpt: fastest\n",
    "2. Stockmark/3. gpt-neox-japanese-1.4b: best answer so far\n",
    "\n",
    "\n",
    "##### for prompt: \"こんにちは\"\n",
    "1. rsesponse from GPT is the most natural one even when we are using completion endpoint instead of chat completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
